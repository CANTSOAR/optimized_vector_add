{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "485b6d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b670c87f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSuppose N vectors with M dimensions\\n\\nM represents the number of factors, in our case the number of sectors that we decide to use\\n\\nThis gives us:\\n\\n > S, the NxM matrix of stocks split up into their factor exposures\\n > E, the MxM covariance matrix across the different factors\\n > I, the NxN diagonal matrix of idiosyncratic risks/variances\\n > V, the Nx1 vector of idiosyncratic variances of the stocks\\n\\nTo calculate the best vector combination of S to get some target vector t\\n\\n1) Find the vector s in S that is closest to pointing in the same direction as error e    [initially, e = t]\\n2) Calculate the new error e by taking e = e - e * s / (s * s)   [we update e by consider how much can be projected onto best vector s and subtracted away]\\n\\n^ This ignores the problem on constraining on something like variance for now\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Suppose N vectors with M dimensions\n",
    "\n",
    "M represents the number of factors, in our case the number of sectors that we decide to use\n",
    "\n",
    "This gives us:\n",
    "\n",
    " > S, the NxM matrix of stocks split up into their factor exposures\n",
    " > E, the MxM covariance matrix across the different factors\n",
    " > I, the NxN diagonal matrix of idiosyncratic risks/variances\n",
    " > V, the Nx1 vector of idiosyncratic variances of the stocks\n",
    "\n",
    "To calculate the best vector combination of S to get some target vector t\n",
    "\n",
    "1) Find the vector s in S that is closest to pointing in the same direction as error e    [initially, e = t]\n",
    "2) Calculate the new error e by taking e = e - e * s / (s * s)   [we update e by consider how much can be projected onto best vector s and subtracted away]\n",
    "\n",
    "^ This ignores the problem on constraining on something like variance for now\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24d880c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "M = 5\n",
    "\n",
    "S = np.random.rand(N, M) * 10 - 5\n",
    "E = np.random.rand(M, N) / 50\n",
    "E = E @ E.T\n",
    "I = np.diag(np.random.rand(N))\n",
    "\n",
    "t = np.random.rand(M) * 10 - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57650e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_vector_sum(vector_library, target, error_bound = 1e-5):\n",
    "    error = target\n",
    "    vector_sum = []\n",
    "\n",
    "    vector_library_copy = vector_library.copy()\n",
    "\n",
    "    while vector_library_copy.any() and np.linalg.norm(error) > error_bound:\n",
    "        alignments = vector_library_copy @ error\n",
    "        most_aligned_idx = np.argmax(abs(alignments))\n",
    "        most_aligned_vector = vector_library[most_aligned_idx]\n",
    "\n",
    "        scaling_coeff = np.dot(most_aligned_vector, error) / np.dot(most_aligned_vector, most_aligned_vector)\n",
    "        vector_sum.append((most_aligned_idx, scaling_coeff))\n",
    "\n",
    "        error = error - scaling_coeff * most_aligned_vector\n",
    "        vector_library_copy[most_aligned_idx] = np.zeros_like(most_aligned_vector)\n",
    "\n",
    "    final_vector = np.zeros_like(error)\n",
    "    for vector_idx, coeff in vector_sum:\n",
    "        final_vector += coeff * vector_library[vector_idx]\n",
    "\n",
    "    return vector_sum, final_vector, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c3591e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target:        [ 2.2685471   1.32373219  3.06853891  4.56703558 -4.59342132]\n",
      "Construction:  [ 2.26854737  1.32373062  3.0685419   4.56703278 -4.59341814]\n",
      "\n",
      " num vecs 19\n"
     ]
    }
   ],
   "source": [
    "print(\"Target:       \", t)\n",
    "raw_sum, final_vec, error = optimize_vector_sum(S, t)\n",
    "print(\"Construction: \", final_vec)\n",
    "\n",
    "print(\"\\n num vecs\", len(raw_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbf99bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNow, to try and include the variance as a topic of optimization, we can define a loss function\\n\\nGiven the prior definitions, when we finally have weightings for each of our stocks, we can represent that as w, the vector of weightings\\n\\nThe risk associated with w can be represented as:\\n\\n > w * S * E * (w * S)T + w * I * w\\n\\nIn english, the square beta-wieghted sum of covariances plus the square weighted idiosyncratic risks\\n\\nSo, when selecting vectors with weights w1, w2, ..., wn, we can calculate the above and then normailize by the square of total weighting\\n\\nTo optimize on this, we can make a loss that is a combinated of the dot product for direction and the risk from above, then take the best option\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now, to try and include the variance as a topic of optimization, we can define a loss function\n",
    "\n",
    "Given the prior definitions, when we finally have weightings for each of our stocks, we can represent that as w, the vector of weightings\n",
    "\n",
    "The risk associated with w can be represented as:\n",
    "\n",
    " > w * S * E * (w * S)T + w * I * w\n",
    "\n",
    "In english, the square beta-wieghted sum of covariances plus the square weighted idiosyncratic risks\n",
    "\n",
    "So, when selecting vectors with weights w1, w2, ..., wn, we can calculate the above and then normailize by the square of total weighting\n",
    "\n",
    "To optimize on this, we can make a loss that is a combinated of the dot product for direction and the risk from above, then take the best option\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c638b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_constrainted_vector_sum(vector_library, target, covariance_matrix, idio_matrix, error_bound = 1e-5):\n",
    "    error = target\n",
    "    vector_sum = np.zeros(vector_library.shape[0])\n",
    "\n",
    "    vector_library_copy = vector_library.copy()\n",
    "    c = 0\n",
    "\n",
    "    while vector_library_copy.any() and np.linalg.norm(error) > error_bound:\n",
    "        all_coeffs = vector_library_copy @ error / (np.einsum(\"ij,ij->i\", vector_library_copy, vector_library_copy) + 1e-8)\n",
    "        possible_weights = np.array([vector_sum for i in all_coeffs]) + np.diag(all_coeffs)\n",
    "        alignments = -abs(all_coeffs) / (error.T @ error) ** (1/2) * (np.einsum(\"ij,ij->i\", vector_library_copy, vector_library_copy) + 1e-8) ** (1/2)\n",
    "        variance = np.diag(possible_weights.T @ vector_library @ covariance_matrix @ (possible_weights.T @ vector_library).T + possible_weights.T @ idio_matrix @ possible_weights) / np.sum(abs(possible_weights), axis = 1)**2\n",
    "        \n",
    "        bias = .75 * np.exp(-c / len(vector_sum))\n",
    "        loss = alignments * (1 - bias) + variance * bias\n",
    "\n",
    "        best_idx = np.argmin(loss)\n",
    "\n",
    "        best_vector = vector_library[best_idx]\n",
    "        scaling_coeff = all_coeffs[best_idx]\n",
    "\n",
    "        vector_sum[best_idx] += scaling_coeff\n",
    "        error = error - scaling_coeff * best_vector\n",
    "\n",
    "        c += 1\n",
    "\n",
    "    final_vector = vector_sum.T @ vector_library\n",
    "\n",
    "    return vector_sum / np.sum(vector_sum), final_vector, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc4a0600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.00000000e+00, -0.00000000e+00, -1.03419582e+02, -0.00000000e+00,\n",
       "        -0.00000000e+00,  7.57174904e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00,  1.62495108e+02, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -0.00000000e+00, -0.00000000e+00,  4.95298376e-02,  4.90799579e+01,\n",
       "        -2.20617324e-03, -0.00000000e+00,  1.40798807e+01, -0.00000000e+00,\n",
       "         5.04904021e-01, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -1.31763038e-02,  8.04237835e-02, -0.00000000e+00, -0.00000000e+00,\n",
       "         3.05442788e+00,  1.01342483e+00, -0.00000000e+00, -1.36710258e-01,\n",
       "        -0.00000000e+00, -1.07329069e-02, -0.00000000e+00, -0.00000000e+00,\n",
       "        -3.33519956e+01, -0.00000000e+00, -0.00000000e+00, -2.64814565e-04,\n",
       "        -9.93768400e-01, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        -9.69116280e-04, -0.00000000e+00]),\n",
       " array([ 2.26854486,  1.32372966,  3.06854159,  4.56703586, -4.59342015]),\n",
       " array([ 2.24084768e-06,  2.53654606e-06, -2.67732167e-06, -2.81389449e-07,\n",
       "        -1.16975188e-06]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize_constrainted_vector_sum(S, t, E, I)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
